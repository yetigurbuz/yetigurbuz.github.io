<tr>
    <td width="30%">
    <img src='./publications/recognition_papers/otbovw.png' width="95%">
    </td>
    <td valign="top" width="70%">
    
    <papertitle>A Novel BoVW mimicking end-to-end trainable CNN classification framework using Optimal Transport Theory</papertitle>
    <br>
    (<a target="_blank" href="https://ieeexplore.ieee.org/document/8803276">ieee.org</a>)
    <br>
    <strong>Y. Z. Gurbuz</strong>, A. A. Alatan,
    <em>IEEE International Conference on Image Processing (ICIP), 2019</em>
    <p></p>
    <p id="textAreaOTBoVW" align="justify" style = "font-size:15px">In this work, we show that the output of the last convolutions layer possess a bag-of-visual-words (BoVW) representation once optimal transport distance is employed for the similarity assessment. Building on that perspective, we consider any patch extracted from an image as a unique visual word and represent the image as the uniform histogram of the visual words with the histogram bins associated to embedding vectors according to the semantic meanings of the corresponding visual words. Thus, in the CNN classification framework, the output of the last convolutional block is considered as the global representation of the image and the embeddings are inherently learned within the classification framework. With the proposed formulation, undesired quantization for the BoVW representation is no more required; moreover, the learned CNN features are naturally interpretable.
    <p><br></p>
    </td>
</tr>