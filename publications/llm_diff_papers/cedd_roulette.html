<tr>
    <td width="30%">
    <img src='./publications/llm_diff_papers/cedd_roulette.png' width="95%">
    </td>
    <td valign="top" width="70%">
    
    <papertitle>Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models</papertitle>
    <br>
    (<a target="_blank" href="https://openreview.net/forum?id=Mri9WIfxSm&noteId=ffzMfpLgU2">openreview.net</a>)
    <br>
    E. Haxholli, <strong>Y. Z. Gurbuz</strong>, O. Can, E. Waxman,
    <em>International Conference on Learning Representations (ICLR) , 2025</em>
    <p></p>
    <p id="textAreaCEDDRoulette" align="justify" style = "font-size:15px">In this work, we introduce new theoretical results and a novel transition-rate matrix for discrete diffusion models in language modeling. Our three new theorems provide tighter bounds on KL divergence and cross-entropy, thereby improving perplexity-based evaluation. We also propose “roulette diffusion,” which interpolates between absorbing and uniform forward processes. Furthermore, we present the “roulette diffusion” transition matrix, which interpolates between “absorbing” and “uniform” forward processes. Unlike purely absorbing schemes, roulette diffusion allows unmasked tokens to be revisited and refined during the reverse process—an important capability for grammar and typo correction, where corrections must be made to tokens already revealed. This refinement mechanism is typically unavailable when the reverse diffusion process is unmasking. We also propose an efficient matrix exponential for roulette diffusion, simplifying both the training and inference steps of discrete diffusion by making the conditional ratios analytically accessible. To further improve training, we revitalize a cross-entropy-based objective (CEDD), which avoids directly learning conditional ratios (as in SEDD) and focuses instead on learning the per-token mixing weights. Not only does CEDD lead to faster training and lower perplexities—demonstrated by empirical results on multiple discrete diffusion setups—but it also gracefully handles the scale variability introduced by unmasked tokens having larger magnitudes than masked tokens. Altogether, these contributions demonstrate that our roulette diffusion matrix, combined with CEDD training, offers a flexible unmasking mechanism and strong modeling performance.
    <p><br></p>
    </td>
</tr>